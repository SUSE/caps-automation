name: "SUSE Private Registry:2.1"
on:
  workflow_dispatch:
  schedule:
    - cron: "0 */3 * * *"

jobs:
  test_suse_private_registry:
    name: Integration Tests
    runs-on: ecosystem-ci-runners
    strategy:
      fail-fast: false
      matrix:
        # devel   -> Devel:CAPS:Registry:2.1
        # suse    -> SUSE:SLE-15-SP2:Update:Products:CAPS-Registry:2.1
        # release -> SUSE:Containers:CAPS:2
        source: [devel, suse, release, release-to-devel, release-to-suse]
        include:
          - source: devel
            install_src: registry.suse.de/devel/caps/registry/2.1
          - source: suse
            install_src: registry.suse.de/suse/sle-15-sp2/update/products/caps-registry/2.1
          - source: release
            install_src: registry.suse.de/suse/containers/caps/2
          - source: release-to-devel
            install_src: registry.suse.de/suse/containers/caps/2
            upgrade_src: registry.suse.de/devel/caps/registry/2.1
          - source: release-to-suse
            install_src: registry.suse.de/suse/containers/caps/2
            upgrade_src: registry.suse.de/suse/sle-15-sp2/update/products/caps-registry/2.1
    steps:
      - name: Setup environment for job triggered by ${{ github.event_name }}
        id: setup
        run: |
          case "${GITHUB_EVENT_NAME}" in
            schedule|workflow_dispatch)
              echo "::set-output name=env_ref::release-2.1"
              echo "::set-output name=env_name::periodic-2.1"
              echo "::set-env name=NAMESPACE::spr21-sched-${{ matrix.source }}"
              ;;
            *)
              echo "::error::Trigger event ${GITHUB_EVENT_NAME} not supported by this workflow"
              exit 1
          esac
      - name: Fetch kubeconfig from active CaaSP cluster
        run: |
          GH_API_HEADER="authorization: Bearer ${{ secrets.GITHUB_TOKEN }}"
          active_cluster_artifacts_url=$(curl -sH "${GH_API_HEADER}" \
            https://api.github.com/repos/${{ github.repository }}/actions/workflows/cluster-rotate.yml/runs\?status\=success \
            | jq -r ".workflow_runs[0].artifacts_url")
          kubconfig_artifact_download_url=$(curl -sH "${GH_API_HEADER}" ${active_cluster_artifacts_url} \
            | jq -rc ".artifacts[] | select(.name | contains(\"deployment-registry-ci\")) | .archive_download_url")
          catapult_deployment_zip="catapult_deployment.zip"
          curl -sLH "${GH_API_HEADER}" $kubconfig_artifact_download_url --output "${catapult_deployment_zip}"
          unzip -jo ${catapult_deployment_zip} kubeconfig
          echo "::set-env name=KUBECONFIG::$(realpath ./kubeconfig)"
      - name: Delete previous deployment
        run: |
          kubectl delete ns ${NAMESPACE} || true
          kubectl create ns ${NAMESPACE}
      - name: Get ingress IP
        run: |
          echo "::set-env name=INGRESS_IP::$(kubectl get svc nginx-ingress-controller -n nginx-ingress -o json | jq -r ".status.loadBalancer.ingress[].ip")"
      - name: Generate helm values file for CI
        run: |
          cat << EOF | tee ${NAMESPACE}.yaml
          suse:
            AcceptBetaEULA: "yes"
          expose:
            ingress:
              hosts:
                core: "${NAMESPACE}.${INGRESS_IP}.nip.io"
          externalURL: "https://${NAMESPACE}.${INGRESS_IP}.nip.io"
          updateStrategy:
            type: Recreate
          internalTLS:
            enabled: false
          imagePullPolicy: Always
          EOF
      - name: Create GitHub deployment
        uses: tallyb/deployments@0.5.0
        id: deployment
        with:
          step: start
          token: ${{ github.token }}
          env: ${{ steps.setup.outputs.env_name }} (${{ matrix.source }})
          ref: ${{ steps.setup.outputs.env_ref }}
      - name: SUSE Private Registry (install)
        env:
          HELM_EXPERIMENTAL_OCI: 1
        run: |
          chart="./harbor"
          rm -rf ${chart}
          helm chart pull ${{ matrix.install_src }}/charts/registry/harbor:latest
          helm chart export ${{ matrix.install_src }}/charts/registry/harbor:latest
          # replace images repository according to source
          sed -i 's,registry.suse.com,${{ matrix.install_src }}/containers,g' ${chart}/values.yaml
          helm install ${NAMESPACE} ${chart} -n ${NAMESPACE} --values ${NAMESPACE}.yaml --timeout 8m --wait
      - name: SUSE Private Registry (upgrade)
        if: matrix.upgrade_src
        env:
          HELM_EXPERIMENTAL_OCI: 1
        run: |
          chart="./harbor"
          rm -rf ${chart}
          helm chart pull ${{ matrix.upgrade_src }}/charts/registry/harbor:latest
          helm chart export ${{ matrix.upgrade_src }}/charts/registry/harbor:latest
          # replace images repository according to source
          sed -i 's,registry.suse.com,${{ matrix.upgrade_src }}/containers,g' ${chart}/values.yaml
          helm upgrade ${NAMESPACE} ${chart} -n ${NAMESPACE} --values ${NAMESPACE}.yaml --timeout 8m --wait
      - name: Run tests
        id: run_tests
        run: |
          set +e
          release_name=${NAMESPACE}
          namespace=${NAMESPACE}
          helm test -n $namespace $release_name --timeout 30m &
          helm_test_pid=$!
          test_pods=$(helm status -n $namespace $release_name -o json | jq -r .hooks[].name)
          until kubectl exec -n $namespace $test_pods "--" pgrep "pybot|robot" &> /dev/null; do
            sleep 1
          done
          while kubectl exec -n $namespace $test_pods "--" pgrep "pybot|robot" &> /dev/null; do
            sleep 1
          done
          kubectl cp -n $namespace $test_pods:/var/lib/harbor-test/output.xml output.xml
          wait $helm_test_pid
          status=$?
          rm -rf test_reports/
          rebot -d test_reports --log api_log.html --report api_report.html --xunit api_xunit.xml output.xml
          echo "::set-output name=has_artifacts::true"
          exit $status
      - name: Collect logs from pods
        if: always()
        run: |
          rm -rf pods_logs
          mkdir pods_logs && cd pods_logs
          for pod in $(kubectl get pods -n ${NAMESPACE} -o json | jq -r ".items[].metadata.name"); do
            mkdir $pod && pushd $pod
            kubectl -n ${NAMESPACE} get pod $pod -o json | jq ".status.containerStatuses[]" > status.info
            for container in $(kubectl -n ${NAMESPACE} get pod $pod -o json | jq -r ".status.containerStatuses[].name"); do
              kubectl -n ${NAMESPACE} logs $pod -c $container > $container.log
            done
            popd
          done
      - name: Archive logs from pods
        if: always()
        uses: actions/upload-artifact@v2
        with:
          name: logs-${{ matrix.source }}
          path: pods_logs/*
      - name: Archive test results
        if: always() && steps.run_tests.outputs.has_artifacts
        uses: actions/upload-artifact@v2
        with:
          name: test_report-${{ matrix.source }}
          path: test_reports/*
      - name: Process test results
        if: always() && steps.run_tests.outputs.has_artifacts
        uses: flaviodsr/junit-report-annotations-action@master
        with:
          path: test_reports/api_xunit.xml
      - name: Update GitHub deployment status
        if: always()
        uses: tallyb/deployments@0.5.0
        with:
          step: finish
          token: ${{ github.token }}
          status: ${{ job.status }}
          env_url: https://${{ env.NAMESPACE }}.${{ env.INGRESS_IP }}.nip.io
          logs: https://${{ env.NAMESPACE }}.${{ env.INGRESS_IP }}.nip.io
          deployment_id: ${{ steps.deployment.outputs.deployment_id }}
